{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b961f69b",
   "metadata": {
    "id": "b961f69b"
   },
   "source": [
    "# 251. REINFORCE Algorithm\n",
    "\n",
    "- Monte-Carlo method를 통해 구한 episodic sample 의 estimated return을 이용하여 policy parameter 𝜃를 update해 나가는 기법\n",
    "\n",
    "- REINFORCE 갱신 규칙\n",
    "\n",
    "$$\\Delta\\theta_t = \\alpha\\nabla_\\theta\\log{\\pi_\\theta}(s, a)G_t$$\n",
    "\n",
    "- 따라서, Loss function 은\n",
    "$$-G_t\\log{\\pi_\\theta}(s, a)$$\n",
    "\n",
    "```\n",
    "                log_prob = torch.log(pi(state_tensor))\n",
    "                selected_log_probs = reward_tensor * \\\n",
    "                        torch.gather(log_prob, 1, action_tensor.unsqueeze(1)).squeeze()\n",
    "                loss = -1 * selected_log_probs.mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c463e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swig 설치 (Box2D와 같은 C/C++ 확장 모듈 빌드 시 필요)\n",
    "# gymnasium 라이브러리 설치 (box2d 환경 지원 포함)\n",
    "# 비디오 녹화 기능에 필요한 X 가상 프레임버퍼(xvfb)와 X11 유틸리티 설치\n",
    "# > tmp : 설치 로그를 tmp 파일로 리다이렉트\n",
    "# 가상 디스플레이(pyvirtualdisplay) 설치 (버전 0.2.*로 고정)\n",
    "# - Colab이나 서버 환경에서 GUI 없이 환경 렌더링/비디오 생성 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a62526",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "qCER-H6cO7Rh",
   "metadata": {
    "id": "qCER-H6cO7Rh"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*4RncZNj1ij5A5eMJpexhrw.png\" width=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814aa4c",
   "metadata": {
    "id": "6814aa4c"
   },
   "source": [
    "### 환경 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085c86bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV_NAME = 'LunarLander-v3'\n",
    "# 환경 생성\n",
    "# 행동 공간(Action space)을 정수 배열로 생성\n",
    "# 예: 행동이 4개면 [0, 1, 2, 3]\n",
    "# 행동 공간 출력\n",
    "# 관측 공간(Observation space)의 형태와 행동 개수 출력\n",
    "# env.observation_space.shape: 관측 값의 차원\n",
    "# env.action_space.n: 가능한 행동의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cc627",
   "metadata": {
    "id": "420cc627"
   },
   "source": [
    "### Policy Network 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8ff55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차별 가능한(differentiable) 정책 파라미터화 pi(a|s, θ) 정의\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, n_actions):\n",
    "        # 입력 차원(*input_dims) → 128개의 은닉 노드\n",
    "        # 128개의 은닉 노드 → 행동 개수(n_actions)\n",
    "    def forward(self, state):\n",
    "        # 첫 번째 층 통과 후 ReLU 활성화 함수 적용\n",
    "        # 두 번째 층 통과 후 Softmax 적용 → 각 행동에 대한 확률 분포 생성\n",
    "# 파라미터 θ 초기화\n",
    "# env.observation_space.shape : 관측 값의 차원 (예: (4,))\n",
    "# env.action_space.n           : 행동의 개수 (예: 2)\n",
    "# 네트워크 구조 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cce56db",
   "metadata": {
    "id": "0cce56db"
   },
   "source": [
    "### hyper-parameters 설정, reward 계산 도우미 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d89884b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률(스텝 사이즈) 설정: 0 < alpha < 1\n",
    "# 할인율(감가율) 설정: 0 < gamma < 1\n",
    "# 최대 학습 에피소드 수 N\n",
    "# 한 번의 업데이트에 사용할 에피소드 수 (배치 크기)\n",
    "# Adam 최적화 알고리즘을 사용하여 정책 네트워크(pi) 학습\n",
    "def discount_rewards(rewards):\n",
    "    # 보상을 역순으로 순회하며 할인 보상 계산\n",
    "    # 계산된 리스트를 역순으로 뒤집어서 원래 시점 순서로 복원\n",
    "    # 보상 정규화 (평균 0, 표준편차 1로 스케일링) → 학습 안정성과 수렴 속도 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9655c",
   "metadata": {
    "id": "48d9655c"
   },
   "source": [
    "### main algorithm 작성 / Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0eebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# While episode n < N do:\n",
    "    # 환경 초기화\n",
    "    # 현재 에피소드의 상태, 보상, 행동 저장용 리스트\n",
    "    # Generate an episode: s0, a0, r0, ... st, at, rt following policy pi(a|s, θ)\n",
    "        # 정책 네트워크에서 행동 확률 분포 계산\n",
    "        # 확률 분포에 따라 행동 샘플링\n",
    "        # 환경에 행동 적용 → 다음 상태, 보상, 종료 여부 반환\n",
    "        # 현재 step의 경험 저장\n",
    "        # 상태 업데이트\n",
    "        # 에피소드 종료 시 처리\n",
    "            # 1) 보상 시퀀스 → 할인 보상(Return)으로 변환\n",
    "            # 2) 배치 크기만큼 데이터가 쌓이면 정책 네트워크 업데이트\n",
    "                # 상태, 보상, 행동 → 텐서 변환\n",
    "                # 정책 로그 확률 계산\n",
    "                # 수행한 행동에 해당하는 로그 확률만 선택 후, 할인 보상 곱하기\n",
    "                # 정책 경사도의 손실 함수: L(θ) = - 평균[ G_t * log π(a|s, θ) ]\n",
    "                # 정책 네트워크 파라미터 업데이트\n",
    "                # 그래디언트 클리핑 (폭주 방지)\n",
    "                # 배치 데이터 초기화\n",
    "    # 100 에피소드마다 최근 100개 평균 보상 출력\n",
    "# 환경 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840559f",
   "metadata": {
    "id": "8840559f"
   },
   "source": [
    "### reward 변화 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f28b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 소요 시간 출력 (분 단위)\n",
    "# 최근 100 에피소드 평균 보상을 저장할 배열 초기화\n",
    "# 각 에피소드별 최근 100개 평균 보상 계산\n",
    "    # i번째까지의 보상 중, 최대 100개 구간의 평균을 계산\n",
    "# 러닝 평균 보상 그래프 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4qxsHHDlOW82",
   "metadata": {
    "id": "4qxsHHDlOW82"
   },
   "source": [
    "### 이미 학습되어 저장된 model load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5d550b",
   "metadata": {
    "id": "2f5d550b"
   },
   "source": [
    "### Animate it with Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동영상(mp4 등)을 base64로 인코딩하기 위한 모듈\n",
    "# Gymnasium 환경에서 에피소드 실행 영상을 녹화하는 래퍼\n",
    "# 가상 디스플레이 시작 (Colab이나 서버 환경에서 영상 렌더링용)\n",
    "# MP4 영상을 HTML로 변환하여 노트북에 표시하는 함수\n",
    "def render_mp4(videopath: str, width: int = 400) -> HTML:\n",
    "# 환경 준비\n",
    "# render_mode='rgb_array'로 설정해야 RecordVideo가 동작 가능\n",
    "# 비디오 기록을 위한 래퍼(wrapper) 적용\n",
    "# video_folder에 저장, episode_trigger=lambda e: True → 모든 에피소드 기록\n",
    "# 행동(action) 공간 설정 (예: [0, 1])\n",
    "# 에피소드 실행\n",
    "    # 현재 상태(state)를 기반으로 정책 네트워크(pi)에서 행동 확률 계산\n",
    "    # 확률 분포(probs)에 따라 행동 선택\n",
    "    # 환경에서 선택한 행동 실행\n",
    "    # 상태 갱신\n",
    "# 환경 종료 (비디오 기록 완료)\n",
    "# 저장된 비디오 중 가장 최근 파일 찾기\n",
    "# Jupyter/Colab에서 비디오 재생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62691c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
