{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9999980a",
   "metadata": {
    "id": "yTfA3OUb3Hcx"
   },
   "source": [
    "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics\n",
    "\n",
    "- Colab에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a6391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole 환경 생성\n",
    "# 환경에서 선택 가능한 행동(action)의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae935c1",
   "metadata": {
    "id": "uciS0nsN3Hc2"
   },
   "source": [
    "## Experience Replay\n",
    "\n",
    "예시 시나리오 (capacity = 4)\n",
    "\n",
    "| step | position | memory                              |\n",
    "| ---- | -------- | ----------------------------------- |\n",
    "| 0    | 0        | \\[exp0]                             |\n",
    "| 1    | 1        | \\[exp0, exp1]                       |\n",
    "| 2    | 2        | \\[exp0, exp1, exp2]                 |\n",
    "| 3    | 3        | \\[exp0, exp1, exp2, exp3]           |\n",
    "| 4    | 0        | \\[exp4, exp1, exp2, exp3] ← 덮어쓰기 시작 |\n",
    "| 5    | 1        | \\[exp4, exp5, exp2, exp3]           |\n",
    "| ...  | ...      | 계속해서 순환 덮어쓰기                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bffe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, capacity):\n",
    "        # 메모리의 최대 저장 용량 설정\n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        # 하나의 transition을 메모리에 저장 (s, a, s', r, done)\n",
    "        # 메모리 용량이 아직 부족하면 append, 가득 찼으면 덮어쓰기\n",
    "        # 저장 위치를 순환시키기 위해 모듈로 연산 사용\n",
    "    def sample(self, batch_size):\n",
    "        # 메모리에서 무작위로 batch_size만큼 샘플링하고, 각 항목별로 묶어서 반환\n",
    "        # zip(*list) 형식으로 반환하면 (states, actions, new_states, rewards, dones) 순으로 반환됨\n",
    "    def __len__(self):\n",
    "        # 현재 메모리에 저장된 transition의 수 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리플레이 메모리 D를 용량 10으로 초기화\n",
    "# 환경을 초기화하고 상태 s를 얻음\n",
    "# 5번의 경험을 저장\n",
    "    # 무작위로 액션 a를 선택\n",
    "    # 환경에 액션 a를 적용하고 다음 상태 s_, 보상 r 등을 얻음\n",
    "    # 에피소드가 끝났는지 여부 확인\n",
    "    # 경험 (s, a, s_, r, done)을 메모리에 저장\n",
    "    # 다음 상태를 현재 상태로 업데이트\n",
    "# 저장된 메모리 내용을 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b6065b",
   "metadata": {
    "id": "rBWTiu01BTOB"
   },
   "source": [
    "## Sample random minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730b30d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2de8bc0e",
   "metadata": {
    "id": "c-xKKrqw3Hc5"
   },
   "source": [
    "## Select Action\n",
    "\n",
    "- 4개의 특성(feature)으로 구성된 상태에서, 각각의 상태에서 선택할 수 있는 행동이 2가지인 환경에서, 신경망으로 근사한 상태-행동 가치 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712f2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 차원, state feature\n",
    "# 출력 차원,  action space\n",
    "# 은닉층의 뉴런 수\n",
    "# Q-함수 근사를 위한 신경망 정의\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        # 첫 번째 선형 계층: 입력 → 은닉층\n",
    "        # 두 번째 선형 계층: 은닉층 → 출력층 (Q값 출력)\n",
    "    def forward(self, x):\n",
    "        # 첫 번째 은닉층에 ReLU 활성화 함수 적용\n",
    "        # 최종 출력값 (각 행동에 대한 Q값) 반환\n",
    "# 네트워크를 생성하고, GPU 사용 가능 시 GPU에 올림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb069c",
   "metadata": {
    "id": "d0fir5Pl3Hc7"
   },
   "source": [
    "- 입력 : 4 개 feature 로 구성된 state\n",
    "- 출력 : 2 개 action values  \n",
    "\n",
    "- greedy action : $max_{a'}Q(s', a';\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b09a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경을 초기화하고, 초기 상태(state) s를 얻음\n",
    "# Q 네트워크에 상태를 입력하여 각 행동(action)에 대한 Q값 계산\n",
    "# 계산된 Q값 출력 (예: [왼쪽으로 이동할 Q값, 오른쪽으로 이동할 Q값])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b568f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# greedy 방식으로 행동 선택 (가장 Q값이 높은 행동 선택)\n",
    "# 선택된 행동 출력 (예: 0 또는 1 → CartPole에서는 왼쪽 또는 오른쪽)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7844e0a9",
   "metadata": {
    "id": "dILVsp1p3Hc9"
   },
   "source": [
    "## State-Action Value (q value) from DQN\n",
    "\n",
    "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
    "\n",
    "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7961b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 형태의 상태들(states)을 텐서로 변환하고 GPU로 이동\n",
    "# Q-네트워크에 상태들을 넣어 각 행동에 대한 Q값 예측\n",
    "# detach(): 그래디언트 추적 중단\n",
    "# cpu(): GPU에 있던 값을 CPU로 이동 (출력/시각화용)\n",
    "# 각 상태에 대한 모든 행동의 Q값 출력\n",
    "# 각 상태별로 가장 큰 Q값과 그에 해당하는 행동 인덱스를 튜플로 출력\n",
    "# 가장 큰 Q값들만 추출 (values: 최대 Q값, indices: 해당 행동 인덱스)\n",
    "# 각 상태에 대해 선택된 행동의 Q값 (가장 큰 값)\n",
    "# 각 상태에 대해 Q값이 가장 큰 행동의 인덱스 (예: 0 또는 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b8a85a",
   "metadata": {
    "id": "oSugiYLx3Hc-"
   },
   "source": [
    "## torch.gather\n",
    "\n",
    "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
    "\n",
    "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
    "\n",
    "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
    "\n",
    "```\n",
    "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
    "```\n",
    "\n",
    "\n",
    "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>\n",
    "\n",
    "| 구분                    | 설명                                                 |\n",
    "| --------------------- | -------------------------------------------------- |\n",
    "| `Output of the model` | 신경망의 출력 → 각 상태(batch)에 대해 가능한 모든 action의 Q값들 (4개씩) |\n",
    "| `Actions taken`       | 각 상태에서 실제로 취한 행동의 인덱스 (예: `[2, 3, 0, 2]`)          |\n",
    "| `Result of gather`    | 각 상태에서 실제 취한 행동에 대한 Q값만 추출한 결과                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22158eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather: 각 row에서 지정한 index(actions) 위치의 값만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f3898e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349c379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade05392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행한 action 들을 LongTensor 형태로 정의하고, (batch_size, 1) 형태로 reshape\n",
    "# 예: batch 내 5개의 상태에서 취한 행동 = [1, 0, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d70d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gather를 통해 각 상태에서 취한 action의 Q값 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae41a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc8db936",
   "metadata": {
    "id": "ufB1JYnC3HdB"
   },
   "source": [
    "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
    "\n",
    "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다.\n",
    "\n",
    "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
    "\n",
    "```python\n",
    "    probs = policy_network(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    next_state, reward = env.step(action)\n",
    "    loss = -m.log_prob(action) * reward\n",
    "    loss.backward()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd4b994",
   "metadata": {
    "id": "hEN8t3cq3HdB"
   },
   "source": [
    "### 방법 1) Categorical(probs) 에서 sampling\n",
    "\n",
    "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696e4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 class 를 sampling 할 상대 확률"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923de5aa",
   "metadata": {
    "id": "TACqKr0Y3HdC"
   },
   "source": [
    "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65726daa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ae06e96",
   "metadata": {
    "id": "muu7DisY3HdC"
   },
   "source": [
    "### 방법 2) np.random.choice 에서 sampling\n",
    "\n",
    "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e352e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4269bbb0",
   "metadata": {
    "id": "13-ePWwd3HdD"
   },
   "source": [
    "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d1b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 step 만에 spisode 종료 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c76ee8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse the array direction for cumsum and then\n",
    "# revert back to the original order\n",
    "# return r - r.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66007177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episodic task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb866270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing task\n",
    "def discount_rewards(rewards):\n",
    "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d67b4a",
   "metadata": {
    "id": "simlDDmD3HdE"
   },
   "source": [
    "### REINFORCE 구현을 위한 Score Function\n",
    "\n",
    "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
    "\n",
    "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
    "\n",
    "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
    "\n",
    "\n",
    "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
    "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
    "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597544dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#probs = policy_network(state)\n",
    "#loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6dc531",
   "metadata": {
    "id": "4ZhEg-H53HdE"
   },
   "source": [
    "## Huber Loss\n",
    "\n",
    "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
    "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
    "    - 모든 지점에서 미분이 가능하다.  \n",
    "    - Outlier에 상대적으로 Robust하다.\n",
    "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8acd603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0756da36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f55ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
