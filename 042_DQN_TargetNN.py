import gymnasium as gym
import matplotlib.pyplot as plt
import math
import random
import time
import torch.nn as nn
import torch.optim as optim
import torch
import matplotlib
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'

env_name = 'MountainCar-v0' 
# env_name = 'CartPole-v1'

# í™˜ê²½ ìƒì„±
env = gym.make(env_name)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
num_episodes = 300
GAMMA = 0.99  # ê°ë§ˆ (discount factor)
learning_rate = 0.001  # í•™ìŠµë¥ 
hidden_layer = 120  # ì€ë‹‰ì¸µ ë…¸ë“œ ìˆ˜
replay_memory_size = 50_000  # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ í¬ê¸°
batch_size = 128  # ë°°ì¹˜ í¬ê¸°

e_start = 0.9  # ìž…ì‹¤ë¡  ì´ˆê¸°ê°’
e_end = 0.05  # ìž…ì‹¤ë¡  ìµœì¢…ê°’
e_decay = 200  # ìž…ì‹¤ë¡  ê°ì†Œìœ¨

target_nn_update_frequency = 10  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°

# ë Œë”ë§ ê´€ë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
render_episodes = 10  # ë Œë”ë§í•  ì—í”¼ì†Œë“œ ìˆ˜
render_start_episode = 290  # ë Œë”ë§ì„ ì‹œìž‘í•  ì—í”¼ì†Œë“œ ë²ˆí˜¸ (ë” ì¼ì° ì‹œìž‘) 

device = "cpu"

n_inputs = env.observation_space.shape[0]  # ìž…ë ¥ ì°¨ì› ìˆ˜ (ìƒíƒœ ìˆ˜)
n_outputs = env.action_space.n  # ì¶œë ¥ ì°¨ì› ìˆ˜ (ì•¡ì…˜ ìˆ˜)

# ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ í´ëž˜ìŠ¤
class ExperienceReplay:
    def __init__(self, capacity):
        self.capacity = capacity  # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì˜ ìµœëŒ€ í¬ê¸° ì„¤ì •
        self.memory = []  # ê²½í—˜ì„ ì €ìž¥í•  ë©”ëª¨ë¦¬ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        self.position = 0  # í˜„ìž¬ ì €ìž¥ ìœ„ì¹˜ ì´ˆê¸°í™”

    # ê²½í—˜ ì¶”ê°€ í•¨ìˆ˜
    def push(self, state, action, new_state, reward, done):
        # ì£¼ì–´ì§„ ê²½í—˜(transition)ì„ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        transition = (state, action, new_state, reward, done)

        if self.position >= len(self.memory):
            # ë©”ëª¨ë¦¬ì— ë¹ˆ ê³µê°„ì´ ìžˆìœ¼ë©´ ê²½í—˜ ì¶”ê°€
            self.memory.append(transition)
        else:
            # ë©”ëª¨ë¦¬ê°€ ê°€ë“ ì°¨ë©´ ì˜¤ëž˜ëœ ê²½í—˜ì„ ë®ì–´ì“°ê¸°
            self.memory[self.position] = transition
            
        # ì €ìž¥ ìœ„ì¹˜ë¥¼ ë‹¤ìŒìœ¼ë¡œ ì´ë™, ìš©ëŸ‰ì„ ì´ˆê³¼í•˜ë©´ ì²˜ìŒìœ¼ë¡œ ëŒì•„ê°
        self.position = (self.position + 1) % self.capacity

    # ê²½í—˜ ìƒ˜í”Œë§ í•¨ìˆ˜
    def sample(self, batch_size):
        # ë©”ëª¨ë¦¬ì—ì„œ ì£¼ì–´ì§„ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ë¬´ìž‘ìœ„ë¡œ ìƒ˜í”Œë§í•˜ì—¬ ë°˜í™˜
        return zip(*random.sample(self.memory, batch_size))

    def __len__(self):
        # í˜„ìž¬ ë©”ëª¨ë¦¬ì— ì €ìž¥ëœ ê²½í—˜ì˜ ìˆ˜ë¥¼ ë°˜í™˜
        return len(self.memory)

# ì‹ ê²½ë§ í´ëž˜ìŠ¤
class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.linear1 = nn.Linear(n_inputs, hidden_layer)
        self.linear2 = nn.Linear(hidden_layer, hidden_layer//2)
        self.linear3 = nn.Linear(hidden_layer//2, n_outputs)

    # ìˆœì „íŒŒ í•¨ìˆ˜
    def forward(self, x):
        a1 = torch.relu(self.linear1(x))
        a2 = torch.relu(self.linear2(a1))
        output = self.linear3(a2)
        return output

# ì•¡ì…˜ ì„ íƒ í•¨ìˆ˜
def select_action(state, steps_done):
    # ìž…ì‹¤ë¡  ê°’ ê³„ì‚°
    e_threshold = e_end + (e_start - e_end) * \
        math.exp(-1. * steps_done/e_decay)

    if random.random() > e_threshold:
        # ìž…ì‹¤ë¡ ë³´ë‹¤ í° ê²½ìš°, Q í•¨ìˆ˜ì— ë”°ë¼ í–‰ë™ ì„ íƒ
        with torch.no_grad():
            state = torch.Tensor(state).to(device)   # ìƒíƒœë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ê³  ìž¥ì¹˜ì— í• ë‹¹
            action_values = Q(state)   # Q í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í–‰ë™ì˜ ê°€ì¹˜ ê³„ì‚°
            action = torch.argmax(action_values).item()   # ê°€ìž¥ ë†’ì€ ê°€ì¹˜ë¥¼ ê°–ëŠ” í–‰ë™ ì„ íƒ
    else: 
        # ìž…ì‹¤ë¡ ë³´ë‹¤ ìž‘ì€ ê²½ìš°, ë¬´ìž‘ìœ„ í–‰ë™ ì„ íƒ (íƒìƒ‰)
        action = env.action_space.sample()

    return action

# ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
memory = ExperienceReplay(replay_memory_size)

# íƒ€ê²Ÿ Qí•¨ìˆ˜ ì´ˆê¸°í™” (ëžœë¤ ê°€ì¤‘ì¹˜)
target_Q = NeuralNetwork().to(device)

# Qí•¨ìˆ˜ ì´ˆê¸°í™” (ëžœë¤ ê°€ì¤‘ì¹˜ë¡œ ì‹ ê²½ë§ ìƒì„±)
Q = NeuralNetwork().to(device)

# ì†ì‹¤ í•¨ìˆ˜ ì„¤ì • (í‰ê·  ì œê³± ì˜¤ì°¨)
criterion = nn.MSELoss()

# ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„¤ì • (Adam ì˜µí‹°ë§ˆì´ì €)
optimizer = optim.Adam(Q.parameters(), lr=learning_rate)

# íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì¹´ìš´í„° ì´ˆê¸°í™”
update_target_counter = 0
# ê° ì—í”¼ì†Œë“œì—ì„œ ì–»ì€ ë³´ìƒì„ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
reward_history = []
# ì´ ìŠ¤í… ìˆ˜ ì´ˆê¸°í™”
total_steps = 0
# í•™ìŠµ ì‹œìž‘ ì‹œê°„ ê¸°ë¡
start_time = time.time()

# ì—í”¼ì†Œë“œ ë£¨í”„
for episode in range(num_episodes):
    # ë Œë”ë§ ì—¬ë¶€ ê²°ì •
    should_render = episode >= render_start_episode
    
    if should_render:
        # ë Œë”ë§ì„ ìœ„í•œ í™˜ê²½ ìƒì„±
        render_env = gym.make(env_name, render_mode="human")
        s, _ = render_env.reset()
        print(f"ðŸŽ¬ Rendering episode {episode} - ë Œë”ë§ ì°½ì´ ì—´ë ¸ìŠµë‹ˆë‹¤!")
    else:
        s, _ = env.reset()
        if episode % 50 == 0:  # 50 ì—í”¼ì†Œë“œë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
            print(f"ðŸ“Š Episode {episode}/{num_episodes} ì§„í–‰ ì¤‘...")

    episode_reward = 0
    while True:
        total_steps += 1

        # ì•¡ì…˜ ì„ íƒ
        a = select_action(s, total_steps)

        # í™˜ê²½ì—ì„œ ì•¡ì…˜ ìˆ˜í–‰
        if should_render:
            s_, r, terminated, truncated, _ = render_env.step(a)
        else:
            s_, r, terminated, truncated, _ = env.step(a)
        done = terminated or truncated
        episode_reward += r

        # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì— ê²½í—˜ ì €ìž¥
        memory.push(s, a, s_, r, done)

        if len(memory) >= batch_size:
            # ë¦¬í”Œë ˆì´ ë©”ëª¨ë¦¬ì—ì„œ ë¯¸ë‹ˆë°°ì¹˜ ìƒ˜í”Œë§
            states, actions, new_states, rewards, dones = memory.sample(
                batch_size)

            # ìƒ˜í”Œë§í•œ ë°ì´í„°ë¥¼ í…ì„œë¡œ ë³€í™˜í•˜ì—¬ ìž¥ì¹˜ì— í• ë‹¹
            states = torch.Tensor(states).to(device)
            actions = torch.LongTensor(actions).to(device)
            new_states = torch.Tensor(new_states).to(device)
            rewards = torch.Tensor(rewards).to(device)  # ì°¨ì› ìˆ˜ì •
            dones = torch.Tensor(dones).to(device)
            
            # íƒ€ê²Ÿ Q ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° ìƒˆë¡œìš´ ìƒíƒœì˜ Q ê°’ ê³„ì‚°
            new_action_values = target_Q(new_states).detach()

            # íƒ€ê²Ÿ ê°’ ê³„ì‚°
            y_target = rewards + \
                (1 - dones) * GAMMA * torch.max(new_action_values, 1)[0]
            # ì˜ˆì¸¡ ê°’ ê³„ì‚°
            y_pred = Q(states).gather(1, actions.unsqueeze(1))

            # ì†ì‹¤ ê³„ì‚° ë° ì—­ì „íŒŒ
            loss = criterion(y_pred.squeeze(), y_target.squeeze())
            optimizer.zero_grad()
            loss.backward()

            optimizer.step()

            # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if update_target_counter % target_nn_update_frequency == 0:
                target_Q.load_state_dict(Q.state_dict())

            update_target_counter += 1

        s = s_

        if done:
            reward_history.append(episode_reward)
            print(f"{episode} episode finished after {episode_reward:.2f} rewards")
            break

    # ë Œë”ë§ í™˜ê²½ì´ ìƒì„±ëœ ê²½ìš° ë‹«ê¸°
    if should_render:
        render_env.close()
        print(f"ðŸ”’ Episode {episode} ë Œë”ë§ ì°½ì„ ë‹«ì•˜ìŠµë‹ˆë‹¤.")

# í™˜ê²½ ë‹«ê¸°
env.close()

# í‰ê·  ë³´ìƒ ì¶œë ¥
print("Average rewards: %.2f" % (sum(reward_history)/num_episodes))

# ë§ˆì§€ë§‰ 50 ì—í”¼ì†Œë“œì˜ í‰ê·  ë³´ìƒ ì¶œë ¥
last_episodes = 50
if len(reward_history) >= last_episodes:
    print(f"Average of last {last_episodes} episodes: %.2f" % (sum(reward_history[-last_episodes:])/last_episodes))
else:
    print(f"Average of all {len(reward_history)} episodes: %.2f" % (sum(reward_history)/len(reward_history)))

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ ì¶œë ¥
print("---------------------- Hyper parameters --------------------------------------")
print(
    f"GAMMA:{GAMMA}, learning rate: {learning_rate}, hidden layer: {hidden_layer}")
print(f"replay_memory: {replay_memory_size}, batch size: {batch_size}")
print(f"epsilon_start: {e_start}, epsilon_end: {e_end}, " +
      f"epsilon_decay: {e_decay}")
print(f"update frequency: {target_nn_update_frequency}")

# ê²½ê³¼ ì‹œê°„ ì¶œë ¥
elapsed_time = time.time() - start_time
print(f"Time Elapsed : {elapsed_time//60} min {elapsed_time%60:.0} sec")

# í•™ìŠµ ê³¼ì •ì˜ ë³´ìƒ í”Œë¡¯
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.bar(range(len(reward_history)), reward_history, alpha=0.6)
plt.xlabel("episodes")
plt.ylabel("rewards")
plt.title("DQN - Target Network (Individual Episodes)")

# ì´ë™ í‰ê·  í”Œë¡¯ ì¶”ê°€
plt.subplot(1, 2, 2)
window_size = 20
if len(reward_history) >= window_size:
    moving_avg = [sum(reward_history[i:i+window_size])/window_size 
                  for i in range(len(reward_history)-window_size+1)]
    plt.plot(range(window_size-1, len(reward_history)), moving_avg, 'r-', linewidth=2)
    plt.xlabel("episodes")
    plt.ylabel("moving average rewards")
    plt.title(f"Moving Average (window={window_size})")

plt.tight_layout()
plt.show()
