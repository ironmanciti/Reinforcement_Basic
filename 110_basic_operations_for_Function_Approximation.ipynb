{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTfA3OUb3Hcx"
      },
      "source": [
        "# 110. Deep Neural Network을 이용한 함수 근사에서 필요한 torch basics\n",
        "\n",
        "- Colab에서 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXD5j_4_3Hc0",
        "outputId": "806325aa-f62a-4ef5-9f46-bf97f11c6ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import gymnasium as gym\n",
        "import collections\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# CartPole 환경 생성\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# 환경에서 선택 가능한 행동(action)의 개수\n",
        "action_size = env.action_space.n\n",
        "action_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uciS0nsN3Hc2"
      },
      "source": [
        "## Experience Replay\n",
        "\n",
        "예시 시나리오 (capacity = 4)\n",
        "\n",
        "| step | position | memory                              |\n",
        "| ---- | -------- | ----------------------------------- |\n",
        "| 0    | 0        | \\[exp0]                             |\n",
        "| 1    | 1        | \\[exp0, exp1]                       |\n",
        "| 2    | 2        | \\[exp0, exp1, exp2]                 |\n",
        "| 3    | 3        | \\[exp0, exp1, exp2, exp3]           |\n",
        "| 4    | 0        | \\[exp4, exp1, exp2, exp3] ← 덮어쓰기 시작 |\n",
        "| 5    | 1        | \\[exp4, exp5, exp2, exp3]           |\n",
        "| ...  | ...      | 계속해서 순환 덮어쓰기                        |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hyL29xvt3Hc3"
      },
      "outputs": [],
      "source": [
        "class ExperienceReplay:\n",
        "    def __init__(self, capacity):\n",
        "        # 메모리의 최대 저장 용량 설정\n",
        "        self.capacity = capacity\n",
        "        self.memory = []         # transition들을 저장할 리스트\n",
        "        self.position = 0        # 현재 저장 위치 인덱스 (순환 구조)\n",
        "\n",
        "    def push(self, state, action, new_state, reward, done):\n",
        "        # 하나의 transition을 메모리에 저장 (s, a, s', r, done)\n",
        "        transition = (state, action, new_state, reward, done)\n",
        "\n",
        "        # 메모리 용량이 아직 부족하면 append, 가득 찼으면 덮어쓰기\n",
        "        if self.position >= len(self.memory):\n",
        "            self.memory.append(transition)\n",
        "        else:\n",
        "            self.memory[self.position] = transition\n",
        "\n",
        "        # 저장 위치를 순환시키기 위해 모듈로 연산 사용\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # 메모리에서 무작위로 batch_size만큼 샘플링하고, 각 항목별로 묶어서 반환\n",
        "        # zip(*list) 형식으로 반환하면 (states, actions, new_states, rewards, dones) 순으로 반환됨\n",
        "        return zip(*random.sample(self.memory, batch_size))\n",
        "\n",
        "    def __len__(self):\n",
        "        # 현재 메모리에 저장된 transition의 수 반환\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mBA3O2s3Hc4",
        "outputId": "a774c908-1146-4126-ae9d-d72b5c6e0f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:\n",
            "   state:      [ 0.04782227  0.01752919 -0.00694257 -0.044804  ]\n",
            "   action:     1\n",
            "   next_state: [ 0.04817285  0.21275    -0.00783865 -0.33966926]\n",
            "   reward:     1.0\n",
            "   done:       False\n",
            "\n",
            "1:\n",
            "   state:      [ 0.04817285  0.21275    -0.00783865 -0.33966926]\n",
            "   action:     1\n",
            "   next_state: [ 0.05242785  0.40798262 -0.01463203 -0.6348137 ]\n",
            "   reward:     1.0\n",
            "   done:       False\n",
            "\n",
            "2:\n",
            "   state:      [ 0.05242785  0.40798262 -0.01463203 -0.6348137 ]\n",
            "   action:     1\n",
            "   next_state: [ 0.0605875   0.6033056  -0.02732831 -0.93206847]\n",
            "   reward:     1.0\n",
            "   done:       False\n",
            "\n",
            "3:\n",
            "   state:      [ 0.0605875   0.6033056  -0.02732831 -0.93206847]\n",
            "   action:     1\n",
            "   next_state: [ 0.07265361  0.7987854  -0.04596968 -1.2332122 ]\n",
            "   reward:     1.0\n",
            "   done:       False\n",
            "\n",
            "4:\n",
            "   state:      [ 0.07265361  0.7987854  -0.04596968 -1.2332122 ]\n",
            "   action:     0\n",
            "   next_state: [ 0.08862932  0.6042837  -0.07063392 -0.95527816]\n",
            "   reward:     1.0\n",
            "   done:       False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 리플레이 메모리 D를 용량 10으로 초기화\n",
        "D = ExperienceReplay(5)\n",
        "\n",
        "# 환경을 초기화하고 상태 s를 얻음\n",
        "s, _ = env.reset()\n",
        "\n",
        "# 5번의 경험을 저장\n",
        "for i in range(5):\n",
        "    # 무작위로 액션 a를 선택\n",
        "    a = env.action_space.sample()\n",
        "\n",
        "    # 환경에 액션 a를 적용하고 다음 상태 s_, 보상 r 등을 얻음\n",
        "    s_, r, truncated, terminated, _ = env.step(a)\n",
        "\n",
        "    # 에피소드가 끝났는지 여부 확인\n",
        "    done = truncated or terminated\n",
        "\n",
        "    # 경험 (s, a, s_, r, done)을 메모리에 저장\n",
        "    D.push(s, a, s_, r, done)\n",
        "\n",
        "    # 다음 상태를 현재 상태로 업데이트\n",
        "    s = s_\n",
        "\n",
        "# 저장된 메모리 내용을 출력\n",
        "for i, (s, a, s_, r, d) in enumerate(D.memory):\n",
        "    print(f\"{i}:\")\n",
        "    print(f\"   state:      {s}\")\n",
        "    print(f\"   action:     {a}\")\n",
        "    print(f\"   next_state: {s_}\")\n",
        "    print(f\"   reward:     {r}\")\n",
        "    print(f\"   done:       {d}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBWTiu01BTOB"
      },
      "source": [
        "## Sample random minibatch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8t4tHzM6BO4_",
        "outputId": "a8c16954-a2c3-45cf-8423-857dfd89831f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------states-------------------------\n",
            "(array([ 0.05242785,  0.40798262, -0.01463203, -0.6348137 ], dtype=float32), array([ 0.04817285,  0.21275   , -0.00783865, -0.33966926], dtype=float32), array([ 0.07265361,  0.7987854 , -0.04596968, -1.2332122 ], dtype=float32), array([ 0.04782227,  0.01752919, -0.00694257, -0.044804  ], dtype=float32), array([ 0.0605875 ,  0.6033056 , -0.02732831, -0.93206847], dtype=float32))\n",
            "-------------actions----------------------\n",
            "(np.int64(1), np.int64(1), np.int64(0), np.int64(1), np.int64(1))\n",
            "------------rewards------------------------\n",
            "(array([ 0.0605875 ,  0.6033056 , -0.02732831, -0.93206847], dtype=float32), array([ 0.05242785,  0.40798262, -0.01463203, -0.6348137 ], dtype=float32), array([ 0.08862932,  0.6042837 , -0.07063392, -0.95527816], dtype=float32), array([ 0.04817285,  0.21275   , -0.00783865, -0.33966926], dtype=float32), array([ 0.07265361,  0.7987854 , -0.04596968, -1.2332122 ], dtype=float32))\n",
            "------------next states--------------------\n",
            "(False, False, False, False, False)\n",
            "---------------dones-------------------------\n",
            "(1.0, 1.0, 1.0, 1.0, 1.0)\n"
          ]
        }
      ],
      "source": [
        "states, actions, rewards, dones, next_states = D.sample(5)\n",
        "\n",
        "print(\"-------------states-------------------------\")\n",
        "print(states)\n",
        "print(\"-------------actions----------------------\")\n",
        "print(actions)\n",
        "print(\"------------rewards------------------------\")\n",
        "print(rewards)\n",
        "print(\"------------next states--------------------\")\n",
        "print(next_states)\n",
        "print(\"---------------dones-------------------------\")\n",
        "print(dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-xKKrqw3Hc5"
      },
      "source": [
        "## Select Action\n",
        "\n",
        "- 4개의 특성(feature)으로 구성된 상태에서, 각각의 상태에서 선택할 수 있는 행동이 2가지인 환경에서, 신경망으로 근사한 상태-행동 가치 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gbVFe-jz3Hc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c63475a-1ccc-46bb-8bd1-4a29ada492dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (linear1): Linear(in_features=4, out_features=64, bias=True)\n",
              "  (linear2): Linear(in_features=64, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# 입력 차원, state feature\n",
        "n_inputs = 4\n",
        "\n",
        "# 출력 차원,  action space\n",
        "n_outputs = 2\n",
        "\n",
        "# 은닉층의 뉴런 수\n",
        "hidden_layer = 64\n",
        "\n",
        "# Q-함수 근사를 위한 신경망 정의\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self) -> None:\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "\n",
        "        # 첫 번째 선형 계층: 입력 → 은닉층\n",
        "        self.linear1 = nn.Linear(n_inputs, hidden_layer)\n",
        "\n",
        "        # 두 번째 선형 계층: 은닉층 → 출력층 (Q값 출력)\n",
        "        self.linear2 = nn.Linear(hidden_layer, n_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 첫 번째 은닉층에 ReLU 활성화 함수 적용\n",
        "        a1 = torch.relu(self.linear1(x))\n",
        "\n",
        "        # 최종 출력값 (각 행동에 대한 Q값) 반환\n",
        "        output = self.linear2(a1)\n",
        "        return output\n",
        "\n",
        "# 네트워크를 생성하고, GPU 사용 가능 시 GPU에 올림\n",
        "Q = NeuralNetwork().to(device)\n",
        "Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0fir5Pl3Hc7"
      },
      "source": [
        "- 입력 : 4 개 feature 로 구성된 state\n",
        "- 출력 : 2 개 action values  \n",
        "\n",
        "- greedy action : $max_{a'}Q(s', a';\\theta)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ1VsEkd3Hc7",
        "outputId": "eeba70a7-c2a1-4b35-f806-9c56e40acf0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1032, 0.0316], device='cuda:0', grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# 환경을 초기화하고, 초기 상태(state) s를 얻음\n",
        "s, _ = env.reset()\n",
        "\n",
        "\n",
        "# Q 네트워크에 상태를 입력하여 각 행동(action)에 대한 Q값 계산\n",
        "action_values = Q(torch.tensor(s).to(device))\n",
        "\n",
        "# 계산된 Q값 출력 (예: [왼쪽으로 이동할 Q값, 오른쪽으로 이동할 Q값])\n",
        "action_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTXrKPAk3Hc8",
        "outputId": "2b5c5746-0c6f-4e26-889e-a348947fb3ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# greedy 방식으로 행동 선택 (가장 Q값이 높은 행동 선택)\n",
        "action = torch.argmax(action_values).item()\n",
        "\n",
        "# 선택된 행동 출력 (예: 0 또는 1 → CartPole에서는 왼쪽 또는 오른쪽)\n",
        "action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dILVsp1p3Hc9"
      },
      "source": [
        "## State-Action Value (q value) from DQN\n",
        "\n",
        "Q-network 에서 입력으로 주어진 states 에 대응하는 action values 를 출력으로 얻어 greedy action 을 선택하는 code.  \n",
        "\n",
        "함수 max()는 최대값과 해당 값의 인덱스를 모두 반환하므로 최대값과 argmax를 모두 계산합니다. 이 경우 값에만 관심이 있기 때문에 결과의 첫 번째 항목(values)을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4FU9vEseM0a",
        "outputId": "f3bde234-b028-4dc5-d993-dedff76eb4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action values:\n",
            "tensor([[-0.0870, -0.0902],\n",
            "        [-0.0316, -0.0338],\n",
            "        [-0.1147, -0.2716],\n",
            "        [ 0.0700,  0.0196],\n",
            "        [-0.1059, -0.1787]])\n",
            "Q values:\n",
            "tensor([-0.0870, -0.0316, -0.1147,  0.0700, -0.1059])\n",
            "actions:\n",
            "tensor([0, 0, 0, 0, 0])\n"
          ]
        }
      ],
      "source": [
        "# numpy 형태의 상태들(states)을 텐서로 변환하고 GPU로 이동\n",
        "states_v = torch.tensor(states).to(device)\n",
        "\n",
        "# Q-네트워크에 상태들을 넣어 각 행동에 대한 Q값 예측\n",
        "# detach(): 그래디언트 추적 중단\n",
        "# cpu(): GPU에 있던 값을 CPU로 이동 (출력/시각화용)\n",
        "action_values = Q(states_v).detach().cpu()\n",
        "\n",
        "# 각 상태에 대한 모든 행동의 Q값 출력\n",
        "print(\"action values:\")\n",
        "print(action_values)\n",
        "\n",
        "# 각 상태별로 가장 큰 Q값과 그에 해당하는 행동 인덱스를 튜플로 출력\n",
        "# 가장 큰 Q값들만 추출 (values: 최대 Q값, indices: 해당 행동 인덱스)\n",
        "values, indices = torch.max(action_values, dim=1)\n",
        "\n",
        "# 각 상태에 대해 선택된 행동의 Q값 (가장 큰 값)\n",
        "print(\"Q values:\")\n",
        "print(values)\n",
        "\n",
        "# 각 상태에 대해 Q값이 가장 큰 행동의 인덱스 (예: 0 또는 1)\n",
        "print(\"actions:\")\n",
        "print(indices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSugiYLx3Hc-"
      },
      "source": [
        "## torch.gather\n",
        "\n",
        "- torch.gather 함수 (또는 torch.Tensor.gather)는 다중 인덱스 선택 방법  \n",
        "\n",
        "- 첫 번째 인수인 input은 요소를 선택하려는 소스 텐서. 두 번째 dim은 수집하려는 차원. 마지막으로 index는 입력을 인덱싱하는 인덱스.\n",
        "\n",
        "4개의 항목과 4개의 작업으로 구성된 일괄 처리가 있는 간단한 예제 사례에서 gather가 수행하는 작업의 요약입니다.\n",
        "\n",
        "```\n",
        "state_action_values = net(states_v).gather(1, actions_v.unsqueeze(1))\n",
        "```\n",
        "\n",
        "\n",
        "<img src=https://miro.medium.com/max/1400/1*fS-9p5EBKVgl69Gy0gwjGQ.png width=400>\n",
        "\n",
        "| 구분                    | 설명                                                 |\n",
        "| --------------------- | -------------------------------------------------- |\n",
        "| `Output of the model` | 신경망의 출력 → 각 상태(batch)에 대해 가능한 모든 action의 Q값들 (4개씩) |\n",
        "| `Actions taken`       | 각 상태에서 실제로 취한 행동의 인덱스 (예: `[2, 3, 0, 2]`)          |\n",
        "| `Result of gather`    | 각 상태에서 실제 취한 행동에 대한 Q값만 추출한 결과                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "output = torch.tensor([[0, 1, 2, 3],\n",
        "                       [4, 5, 6, 7],\n",
        "                       [8, 9,10,11],\n",
        "                       [12,13,14,15]])  # (4, 4)\n",
        "\n",
        "actions = torch.tensor([2, 3, 0, 2]).unsqueeze(1)  # shape: (4, 1)\n",
        "\n",
        "# gather: 각 row에서 지정한 index(actions) 위치의 값만 추출\n",
        "result = output.gather(1, actions)\n",
        "\n",
        "print(result.squeeze())  # tensor([2, 7, 8, 14])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIbB_5QWjiYD",
        "outputId": "950f1ee2-2e7e-418d-d93e-a5980e78fcf4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 2,  7,  8, 14])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSGQnQa3Hc_",
        "outputId": "599cbdb8-6b1a-4060-907c-2d64e215a393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0524,  0.4080, -0.0146, -0.6348],\n",
              "        [ 0.0482,  0.2128, -0.0078, -0.3397],\n",
              "        [ 0.0727,  0.7988, -0.0460, -1.2332],\n",
              "        [ 0.0478,  0.0175, -0.0069, -0.0448],\n",
              "        [ 0.0606,  0.6033, -0.0273, -0.9321]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "states_v  # 4개의 feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD-mRUsViTIq",
        "outputId": "b807c87d-8e7e-45f2-c4ba-05bb9fce4772"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0870, -0.0902],\n",
              "        [-0.0316, -0.0338],\n",
              "        [-0.1147, -0.2716],\n",
              "        [ 0.0700,  0.0196],\n",
              "        [-0.1059, -0.1787]], device='cuda:0', grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "q_values = Q(states_v)\n",
        "q_values  # 2 개의 action values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0iyPk9tiTEc",
        "outputId": "455e42b5-f0f6-409e-f83a-56736d644438"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1],\n",
              "        [0],\n",
              "        [1],\n",
              "        [1],\n",
              "        [0]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# 실행한 action 들을 LongTensor 형태로 정의하고, (batch_size, 1) 형태로 reshape\n",
        "# 예: batch 내 5개의 상태에서 취한 행동 = [1, 0, 1, 1, 0]\n",
        "action = torch.LongTensor([1, 0, 1, 1, 0]).unsqueeze(1).to(device)\n",
        "action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWMhG-8ciS_o",
        "outputId": "f34f73fc-e535-472f-cb9b-44e6487cf6e7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0902],\n",
              "        [-0.0316],\n",
              "        [-0.2716],\n",
              "        [ 0.0196],\n",
              "        [-0.1059]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# # gather를 통해 각 상태에서 취한 action의 Q값 추출\n",
        "torch.gather(q_values, 1, action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCwYsuNJiS3E",
        "outputId": "777a30d3-32fa-442b-eee2-1f8dda25c597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0902],\n",
              "        [-0.0316],\n",
              "        [-0.2716],\n",
              "        [ 0.0196],\n",
              "        [-0.1059]], device='cuda:0', grad_fn=<GatherBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "q_values.gather(1, action)   # 위와 동일 operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufB1JYnC3HdB"
      },
      "source": [
        "## REINFORECE 알고리즘 지원을 위한 PROBABILITY DISTRIBUTIONS - TORCH.DISTRIBUTIONS\n",
        "\n",
        "- distribution 패키지에는 매개변수화할 수 있는 확률 분포와 sampling 함수가 포함되어 있습니다. 이를 통해 최적화를 위한 확률적 계산 그래프 및 확률적 기울기 추정기를 구성할 수 있습니다.\n",
        "\n",
        "- torch 는 다음과 같이 REINFORCE 알고리즘을 지원합니다.\n",
        "\n",
        "```python\n",
        "    probs = policy_network(state)\n",
        "    m = Categorical(probs)\n",
        "    action = m.sample()\n",
        "    next_state, reward = env.step(action)\n",
        "    loss = -m.log_prob(action) * reward\n",
        "    loss.backward()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEN8t3cq3HdB"
      },
      "source": [
        "### 방법 1) Categorical(probs) 에서 sampling\n",
        "\n",
        "'probs'가 길이가 'K'인 1차원 array인 경우, 각 element 는 해당 인덱스에서 클래스를 샘플링할 상대 확률입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRFyebJE3HdB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "logits = torch.rand(4)\n",
        "probs = F.softmax(logits, dim=-1)\n",
        "print(f\"softmax 확률 분포 : {probs}, sum = {probs.sum()}\")\n",
        "\n",
        "# 각 class 를 sampling 할 상대 확률\n",
        "m = Categorical(probs)\n",
        "m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TACqKr0Y3HdC"
      },
      "source": [
        "위의 m 에서 sampling 을 반복하면 softmax 확률 분포로 sampling 된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHvhhzZO3HdC"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = m.sample()\n",
        "    samples.append(a.item())\n",
        "\n",
        "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muu7DisY3HdC"
      },
      "source": [
        "### 방법 2) np.random.choice 에서 sampling\n",
        "\n",
        "- np.random.choice 의 `parameter p`에 softmax 확률 분포 지정하여 sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewhGZw5w3HdC"
      },
      "outputs": [],
      "source": [
        "samples = []\n",
        "\n",
        "for _ in range(30000):\n",
        "    a = np.random.choice(4, p=probs.numpy())\n",
        "    samples.append(a)\n",
        "\n",
        "[cnt/len(samples) for a, cnt in sorted(Counter(samples).items())]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13-ePWwd3HdD"
      },
      "source": [
        "### REINFORCE 구현을  위해  total expected return $G_t$ 를 estimate 하는 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dA4SgEC23HdD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 5 step 만에 spisode 종료 가정\n",
        "rewards = [1, 2, 3, 4, 5]\n",
        "gamma = 0.99"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgyPm6P-3HdD"
      },
      "outputs": [],
      "source": [
        "G_0 = 1 + 0.99**1 * 2 + 0.99**2 * 3 + 0.99**3 * 4 + 0.99**4 * 5\n",
        "G_1 = 2 + 0.99**1 * 3 + 0.99**2 * 4 + 0.99**3 * 5\n",
        "G_2 = 3 + 0.99**1 * 4 + 0.99**2 * 5\n",
        "G_3 = 4 + 0.99**1 * 5\n",
        "G_4 = 5\n",
        "print(G_0, G_1, G_2, G_3, G_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcFvCy3I3HdD"
      },
      "outputs": [],
      "source": [
        "r = np.array([gamma**i * rewards[i] for i in range(len(rewards))])\n",
        "# Reverse the array direction for cumsum and then\n",
        "# revert back to the original order\n",
        "r = r[::-1].cumsum()[::-1]\n",
        "# return r - r.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkFx4vW13HdD"
      },
      "outputs": [],
      "source": [
        "# episodic task\n",
        "Returns = []\n",
        "G = 0\n",
        "for r in rewards[::-1]:\n",
        "    G = r + gamma * G\n",
        "    Returns.append(G)\n",
        "\n",
        "Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlrV5LBC3HdE"
      },
      "outputs": [],
      "source": [
        "# continuing task\n",
        "def discount_rewards(rewards):\n",
        "    Returns = []\n",
        "    G = 0\n",
        "    for r in rewards[::-1]:\n",
        "        G = r + gamma * G\n",
        "        Returns.append(G)\n",
        "    # cumsum의 배열 방향을 반대로 한 다음 원래 순서로 되돌립니다.\n",
        "    Returns = np.array(Returns[::-1], dtype=np.float64)\n",
        "    print(Returns)\n",
        "    return Returns - Returns.mean()\n",
        "\n",
        "discount_rewards(rewards)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simlDDmD3HdE"
      },
      "source": [
        "### REINFORCE 구현을 위한 Score Function\n",
        "\n",
        "- 확률 밀도 함수가 매개 변수와 관련하여 미분할 수있는 경우 REINFORCE를 구현하려면 sample () 및 log_prob () 만 필요\n",
        "\n",
        "$$\\Delta_{\\theta} = \\alpha r \\frac{\\partial log p(a | \\pi^{\\theta}(s))}{\\partial\\theta}$$  \n",
        "\n",
        "$\\alpha$ - learning rate, r - reward,  $p(a|\\pi^\\theta(s))$ - probability of taking action a  \n",
        "\n",
        "\n",
        "- Network 출력에서 action을 샘플링하고 이 action을 environment에 적용한 다음 log_prob를 사용하여 동등한 손실 함수를 구성.   \n",
        "- optimizer는 경사 하강법을 사용하기 때문에 음수를 사용하는 반면 위의 규칙은 경사 상승을 가정.   \n",
        "- Categorical Policy를 사용하는 경우 REINFORCE를 구현하는 코드는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhVDiCjk3HdE"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "s = env.reset()\n",
        "\n",
        "#probs = policy_network(state)\n",
        "logits = torch.rand(2)\n",
        "probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "m = Categorical(probs)\n",
        "action = m.sample()\n",
        "\n",
        "next_state, reward, done, _, _ = env.step(action.item())\n",
        "\n",
        "loss = -m.log_prob(action) * reward\n",
        "#loss.backward()\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZhEg-H53HdE"
      },
      "source": [
        "## Huber Loss\n",
        "\n",
        "- Actor-Critic 의 critic value function 의 loss 계산에 사용  \n",
        "- Huber Loss는 L1과 L2의 장점을 취하면서 단점을 보완하기 위해서 제안된 것이 Huber Loss다.\n",
        "    - 모든 지점에서 미분이 가능하다.  \n",
        "    - Outlier에 상대적으로 Robust하다.\n",
        "<img src=https://bekaykang.github.io/assets/img/post/201209-2.png width=300>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn8JMIvJ3HdF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "curr_q = torch.FloatTensor([10,11,12,10,9])\n",
        "target_q = torch.FloatTensor([12,8,10,13,11])\n",
        "\n",
        "loss = F.smooth_l1_loss(curr_q, target_q)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxtzHqRFeM0n"
      },
      "outputs": [],
      "source": [
        "F.mse_loss(curr_q, target_q)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73w6HEA-eM0o"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}